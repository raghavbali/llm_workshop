{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4a39ae-3cbf-4ee7-878c-79af5d013d56",
   "metadata": {},
   "source": [
    "# Supercharge LLM Apps with DSPy and Langfuse\n",
    "\n",
    "Prompt engineering, the art of crafting precise instructions for LLMs, can be a time-consuming and iterative process. Debugging and troubleshooting LLM behavior can also be complex, given the inherent \"black box\" nature of these models. Additionally, gaining insights into the performance and cost implications of LLM applications is crucial for optimization and scalability (key components for any production grade setup).\n",
    "\n",
    "## The LLM Ecosystem\n",
    "The ecosystem for LLMs is still in its nascent stages. To address some of these challenges, a number of innovative tools and frameworks are being developed. DSPy from Stanford University is one such unique take towards formalizing LLM-based app development. Langfuse on the other-hand has emerged as an offering to streamline and operationalize aspects of LLM app maintenance. To put it in brief:Â \n",
    "- **[DSPY](https://dspy-docs.vercel.app/)** provides a modular and composable framework for building LLM applications, abstracting away the complexities of prompt engineering and enabling developers to focus on the core logic of their applications.\n",
    "- **[Langfuse](https://langfuse.com/docs)** offers a comprehensive observability platform for LLM apps, providing deep insights into model performance, cost, and user interactions.\n",
    "\n",
    "By combining DSPy and Langfuse, developers can unlock the full potential of LLMs, building robust, scalable, and insightful applications that deliver exceptional user experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4d29a0-a37b-41fe-9c8e-b4cb858e5af4",
   "metadata": {},
   "source": [
    "### Langfuse Setup\n",
    "We will make use of self-hosting option for Langfuse. This is based on ``docker`` and ``docker compose``.\n",
    "Steps:\n",
    "- Clone the langfuse repository: ``git clone https://github.com/langfuse/langfuse.git``\n",
    "- From the langfuse repository: ``cd langfuse``\n",
    "- Start the docker containers: ``docker compose up``\n",
    "> The last step spins up a container for langfuse and another one for postgres, you may change settings using the ``.env`` or ``docker-compose.yml`` files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f3ae6-0f65-4c98-8d60-3bc2b15855b5",
   "metadata": {},
   "source": [
    "### Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651dd12-e05b-4750-b02c-f64aca5d0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install dspy-ai=2.5.2\n",
    "# !pip3 install langfuse==2.51.2\n",
    "# pip3 install chromadb==0.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38b72fd8-1510-4bc4-8c74-20165e6e9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import dspy\n",
    "from dsp.utils import deduplicate\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "from dsp.trackers.langfuse_tracker import LangfuseTracker\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from langfuse import Langfuse\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "from scraper_utils import NB_Markdown_Scraper\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4657c235-76f0-4e36-a12a-37b4b1f01873",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'LANGFUSE_PUBLIC_KEY': 'XXXX',\n",
    "    'LANGFUSE_SECRET_KEY': 'XXXX',\n",
    "    'LANGFUSE_HOST': 'http://localhost:3000',\n",
    "    'OPENAI_API_KEY': 'XXXX',\n",
    "    'OPENAI_BASE_URL': '',\n",
    "    'OPENAI_PROVIDER': '',\n",
    "    'CHROMA_DB_PATH': './chromadb/',\n",
    "    'CHROMA_COLLECTION_NAME':\"supercharged_workshop_collection\",\n",
    "    'CHROMA_EMB_MODEL': 'all-MiniLM-L6-v2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d991f5e-ed34-4c32-8ea9-8ee070f1a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = config.get('LANGFUSE_PUBLIC_KEY')\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = config.get('LANGFUSE_SECRET_KEY')\n",
    "os.environ[\"LANGFUSE_HOST\"] = config.get('LANGFUSE_HOST')\n",
    "os.environ[\"OPENAI_API_KEY\"] = config.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff6ee91-2661-4697-8b73-a77f6747bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Langfuse tracker\n",
    "langfuse_tracker = LangfuseTracker(session_id='supercharger001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cdcd002-9eae-4b9a-91be-e5204b0f8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate language-model for DSPY\n",
    "llm_model = dspy.OpenAI(\n",
    "    api_key=config.get('OPENAI_API_KEY'),\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de99e6-b0c5-4c36-b4de-2ffdb45ee94f",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8151ff1b-9660-4c1f-a42d-199ad6dd576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_scraper = NB_Markdown_Scraper([f'../module_0{i}' for i in range(1,5)])\n",
    "nb_scraper.scrape_markdowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba7cdde6-8a0d-404e-a7ab-fb018fb7c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dspy_content.tsv\", \"w\") as record_file:\n",
    "    for k,v in nb_scraper.notebook_md_dict.items():\n",
    "        record_file.write(f\"{k}\\t{v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63769ba8-3eae-42ad-a248-a2675daf2a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = []\n",
    "ctr = 1\n",
    "for k,_ in nb_scraper.notebook_md_dict.items():\n",
    "    doc_ids.append(f'{ctr}_{k}')\n",
    "    ctr+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd03a7-e17a-46c4-af1b-ccb10c078cc8",
   "metadata": {},
   "source": [
    "### Ingest Data into ChromaDB\n",
    "> ensure Chroma is running in our terminal\n",
    "> ``$>chroma run --path ./chromadb``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3baf1c5c-5bdc-4454-981e-2584433e4538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghav.bali/.pyenv/versions/3.11.9/envs/datahack/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chroma_emb_fn = embedding_functions.\\\n",
    "                    SentenceTransformerEmbeddingFunction(\n",
    "                        model_name=config.get(\n",
    "                            'CHROMA_EMB_MODEL'\n",
    "                        )\n",
    "                    )\n",
    "client = chromadb.HttpClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbdeb484-ae0d-401c-b8f0-f16d9c59b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if collection exists\n",
    "collection = client.get_collection(config.get('CHROMA_COLLECTION_NAME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f12287-4d89-4b7a-b5e3-e04c8ba044ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\n",
    "    config.get('CHROMA_COLLECTION_NAME'),\n",
    "    embedding_function=chroma_emb_fn,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93408574-7a22-45fc-82ba-d14feba27b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to collection\n",
    "collection.add(\n",
    "    documents=[v for _,v in nb_scraper.notebook_md_dict.items()], \n",
    "    ids=doc_ids, # must be unique for each doc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a44fe-1a49-402b-996c-3f5e518e5161",
   "metadata": {},
   "source": [
    "### Test Retrieval using ChromaDB Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78abd266-b272-4fae-a1f4-6260e2903e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6_module_03_03_RLHF_phi2', '10_module_04_06_supercharge_llm_apps', '2_module_01_02_getting_started']\n",
      "[0.6175035195275418, 0.7261012146561765, 0.8062081214907408]\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"RLHF\"], # Chroma will embed using the function we provided\n",
    "    n_results=3 # how many results to return\n",
    ")\n",
    "print(results['ids'][0])\n",
    "print(results['distances'][0])\n",
    "#print([i[:100] for j in results['documents'] for i in j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd096119-7012-4154-92fd-97ac8b94c7f6",
   "metadata": {},
   "source": [
    "### Setup ChromaDB as DSPy Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb9c5a1-1a56-49c6-a2f4-79dd4c3bc003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Document__::# Quick Overview of RLFH\n",
       "\n",
       "The performance of Language Models until GPT-3 was kind of amazing as-is. ... \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       ">- __Document id__::6_module_03_03_RLHF_phi2 \n",
       ">- __Document score__::0.6174977412306334"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Document__::... \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       ">- __Document id__::10_module_04_06_supercharge_llm_apps \n",
       ">- __Document score__::0.7260969660795557"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Document__::# Getting Started : Text Representation\n",
       "<img src=\"./assets/banner_notebook_1.jpg\">\n",
       "\n",
       "\n",
       "The NLP domain ... \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       ">- __Document id__::2_module_01_02_getting_started \n",
       ">- __Document score__::0.8062083377747705"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Document__::# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_w... \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       ">- __Document id__::3_module_02_02_simple_text_generator \n",
       ">- __Document score__::0.8826038964887366"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Document__::# <img src=\"./assets/dspy_logo.png\" width=\"2%\"> DSPy: Beyond Prompting\n",
       "---\n",
       "<img src=\"./assets/dspy_b... \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       ">- __Document id__::12_module_04_05_dspy_demo \n",
       ">- __Document score__::0.9200280698248913"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever_model = ChromadbRM(\n",
    "    config.get('CHROMA_COLLECTION_NAME'),\n",
    "    config.get('CHROMA_DB_PATH'),\n",
    "    embedding_function=chroma_emb_fn,\n",
    "    client=client,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "# Test Retrieval\n",
    "results = retriever_model(\"RLHF\")\n",
    "for result in results:\n",
    "    display(Markdown(f\"__Document__::{result.long_text[:100]}... \\n\"))\n",
    "    display(Markdown(f\">- __Document id__::{result.id} \\n>- __Document score__::{result.score}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c3643-bc0a-4f73-a32c-a846bd3e5882",
   "metadata": {},
   "source": [
    "## Prepare DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75724e2f-266b-4b10-9c93-c8186ede6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LM and RM\n",
    "dspy.settings.configure(lm=llm_model,rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93e83804-a6d4-4a48-9924-294628eb3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often less than 50 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b93559-dc08-4b2c-a24b-7539c543ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1a3ba6-ce4f-4692-9732-6c77b7714c32",
   "metadata": {},
   "source": [
    "## Let us Answer Some Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5dd88f39-fcca-43fd-8aee-de19ddb17954",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f6ac3c3-e084-4a0a-8862-adc0184ed782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Question__: List the models covered in module03"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Predicted Answer__: _The models covered in module 03 include LLaMA 3.1, Chinchilla, and Gopher._"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Retrieved Contexts (truncated):__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. # Scaling Neural Nets and Efficient Training\n",
      "\n",
      "We have covered quite some ground in previous 2 modules and observed the steady increase in size and performance of the models. These gains come at huge c...\n",
      "\n",
      "2. # Prompt Engineering\n",
      "<img src=\"./assets/pe_banner.jpg\">\n",
      "\n",
      "Prompt Engineering is this thrilling new discipline that opens the door to a world of possibilities with large language models (LLMs).\n",
      "\n",
      "As a pr...\n",
      "\n",
      "3. # Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop/blob/main/module_02/02_simple_text_generator.ipynb\">\n",
      "  <img src=\"https://colab.research.goog...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Question__: Brief summary of module02"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Predicted Answer__: _Module 02 focuses on text generation using pre-trained models like GPT-2, explaining foundation models, decoding strategies (greedy, beam search, sampling), and the impact of temperature on randomness. It also discusses limitations like long-range context and hallucination._"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Retrieved Contexts (truncated):__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. # Prompt Engineering\n",
      "<img src=\"./assets/pe_banner.jpg\">\n",
      "\n",
      "Prompt Engineering is this thrilling new discipline that opens the door to a world of possibilities with large language models (LLMs).\n",
      "\n",
      "As a pr...\n",
      "\n",
      "2. # Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop/blob/main/module_02/02_simple_text_generator.ipynb\">\n",
      "  <img src=\"https://colab.research.goog...\n",
      "\n",
      "3. # Scaling Neural Nets and Efficient Training\n",
      "\n",
      "We have covered quite some ground in previous 2 modules and observed the steady increase in size and performance of the models. These gains come at huge c...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Question__: What is LLaMA?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Predicted Answer__: _LLaMA is a language model from Meta.AI, available in sizes 8B, 70B, and 405B, and it outperforms many existing LLMs on various benchmarks._"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Retrieved Contexts (truncated):__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. # Open Source Vs Close Sourced LLMs\n",
      "\n",
      "Similar to any other piece of technology, LLMs are available in all flavours and license types. While some of the most popular offerings are closed source (OpenAI ...\n",
      "\n",
      "2. # Scaling Neural Nets and Efficient Training\n",
      "\n",
      "We have covered quite some ground in previous 2 modules and observed the steady increase in size and performance of the models. These gains come at huge c...\n",
      "\n",
      "3. # Retrieval Augmented LLM App\n",
      "<img src=\"./assets/rap_banner.jpeg\">\n",
      "\n",
      "We have covered quite some ground in terms of understanding and building components for:\n",
      "- Text Representation\n",
      "- NLP Tasks\n",
      "- Pretrai...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_questions = [\n",
    "    \"List the models covered in module03\",\n",
    "    \"Brief summary of module02\",\n",
    "    \"What is LLaMA?\"\n",
    "]\n",
    "\n",
    "for question in my_questions:\n",
    "    # Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "    pred = compiled_rag(question)\n",
    "    \n",
    "    display(Markdown(f\"__Question__: {question}\"))\n",
    "    display(Markdown(f\"__Predicted Answer__: _{pred.answer}_\"))\n",
    "    display(Markdown(\"__Retrieved Contexts (truncated):__\"))\n",
    "    for idx,cont in enumerate(pred.context):\n",
    "        print(f\"{idx+1}. {cont[:200]}...\" )\n",
    "        print()\n",
    "    display(Markdown('---'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d3f76da-c5f0-49c6-8a30-3654b0e526d2",
   "metadata": {},
   "source": [
    "## Langfuse\n",
    "Understanding Costs\n",
    "\n",
    "<img src ='./assets/langfuse_dashboard.png'>\n",
    "\n",
    "---\n",
    "\n",
    "<img src = './assets/langfuse_traces.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2ca40-dd85-4eaa-8f74-492e48149421",
   "metadata": {},
   "source": [
    "## Testing Langfuse Dataset using OpenLLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d259e844-2444-4568-887e-a656d464fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse =langfuse_tracker.langfuse\n",
    "ollama_dspy = dspy.OllamaLocal(model='llama3.1',temperature=0.5)\n",
    "\n",
    "# Set up the ollama as LM and RM\n",
    "dspy.settings.configure(lm=ollama_dspy,rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e41de51-1925-4b55-8ac6-5e79c530ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get annotated dataset\n",
    "annotated_dataset = langfuse.get_dataset(\"llm_workshop_rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9b60273-b9d7-4c91-ada8-e30a3d8391d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test rag using ollama\n",
    "ollama_rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "116ccd33-2c0b-47fe-9cc1-bfef0f8de96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__Question__: Brief summary of module02"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Predicted Answer (LLaMA 3.1)__: Here is a brief summary of module02:\n",
       "\n",
       "* LoRA (Low-Rank Adaptation) technique for fine-tuning large models:\n",
       "\t+ Freezes base model weights\n",
       "\t+ Decomposes weight update matrix into lower rank matrices, reducing updates by 100-1000x\n",
       "* qLoRA: Combines quantization and LoRA to further improve efficiency\n",
       "* Model Parameters:\n",
       "\t+ Model size: 405 billion parameters\n",
       "\t+ Training dataset: 15 trillion data points\n",
       "* GPU Performance and Compute Time:\n",
       "\t+ Compute required for training large models\n",
       "\t+ Cost of training large models\n",
       "* Scaling Laws:\n",
       "\t+ Insights from the paper \"Scaling Laws for Neural Language Models\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       ">__Annotated Answer (GPT-4o-mini)__: _Module 02 focuses on text generation using pre-trained models like GPT-2, explaining foundation models, decoding strategies (greedy, beam search, sampling), and the impact of temperature on randomness. It also discusses limitations like long-range context and hallucination._"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Question__: What is LLaMA?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Predicted Answer (LLaMA 3.1)__: It seems like you're trying to follow along with a workshop on Large Language Models (LLMs) and their applications. However, the question about LLaMA was not fully answered.\n",
       "\n",
       "To provide a complete answer:\n",
       "\n",
       "Llama is a large language model developed by Meta AI. It's designed for natural language processing tasks such as text generation, translation, and more. Like other popular LLMs like BERT and RoBERTa, Llama uses self-supervised learning to learn patterns in language from vast amounts of text data.\n",
       "\n",
       "Now, let's get back to the original question: \"Fine-Tuning PEFT - SFT and LLM Landscape - Vector Databases - Libraries and Frameworks\".\n",
       "\n",
       "To answer this question:\n",
       "\n",
       "The topic seems to be"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       ">__Annotated Answer (GPT-4o-mini)__: _LLaMA is a language model from Meta.AI, available in sizes 8B, 70B, and 405B, and it outperforms many existing LLMs on various benchmarks._"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in annotated_dataset.items:\n",
    "    question = item.input[0]['content'].split('Question: ')[-1].split('\\n')[0]\n",
    "    answer = item.expected_output['content'].split('Answer: ')[-1]\n",
    "    o_pred = ollama_rag(question)\n",
    "    with item.observe(\n",
    "        run_name='ollama_experiment',\n",
    "        run_description='compare LLaMA3.1 RAG vs GPT4o-mini RAG ',\n",
    "        run_metadata={\"model\": \"llama3.1\"},\n",
    "    ) as trace_id:\n",
    "        langfuse.score(\n",
    "            name=\"visual-eval\",\n",
    "            # any float value\n",
    "            value=1.0,\n",
    "            comment=\"LLaMA3.1 is very verbose\",\n",
    "        )\n",
    "    langfuse.trace(input=question,output=o_pred.answer,metadata={'model':'LLaMA3.1'})\n",
    "    display(Markdown(f\"__Question__: {question}\"))\n",
    "    display(Markdown(f\"__Predicted Answer (LLaMA 3.1)__: {o_pred.answer}\"))\n",
    "    display(Markdown(f\">__Annotated Answer (GPT-4o-mini)__: _{answer}_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d5e1a-7713-4056-b4af-e8f9363cd1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
