{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfJ4nkINvlan"
   },
   "source": [
    "# Prompt Engineering\n",
    "<img src=\"./assets/pe_banner.jpg\">\n",
    "\n",
    "Prompt Engineering is this thrilling new discipline that opens the door to a world of possibilities with large language models (LLMs).\n",
    "\n",
    "As a prompt engineer, you'll delve into the depths of LLMs, unraveling their capabilities and limitations with finesse. But prompt engineering isn't about mere prompts. It is aa combination of skills and techniques, enabling you to interact and innovate through the use of LLMs.\n",
    "\n",
    "In this module, we will step into the fascinating world of prompt engineering, where we will learn about key principals of working with LLMs through prompts.\n",
    "\n",
    "## Local Model using GPT4ALL\n",
    "> GPT4All is an open-source software ecosystem that allows anyone to train and deploy powerful and customized large language models (LLMs) on everyday hardware. Nomic AI oversees contributions to the open-source ecosystem ensuring quality, security and maintainability.\n",
    "\n",
    "It provides easy to setup and use python bindings.\n",
    "\n",
    "```python\n",
    "!pip install gpt4all\n",
    "```\n",
    "\n",
    "For OpenAI bindings\n",
    "```python\n",
    "!pip install --upgrade openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using version \u001b[39;1m^1.37.0\u001b[39;22m for \u001b[36mstreamlit\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(4.1s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(0.3s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(3.0s)\u001b[39;22m\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(3.3s)\u001b[39;22m\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!poetry add streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop_dhs23/blob/main/module_04/prompt_engineeering_and_langchain.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nAV5vV3Nfpej"
   },
   "outputs": [],
   "source": [
    "import gpt4all\n",
    "from IPython.display import display, Markdown\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_Y4Voy_Gfpel"
   },
   "outputs": [],
   "source": [
    "# NOTE: If you have access to openAI, this can be easily used with the same\n",
    "MODEL_TYPE_OPENAI = 'OPENAI'\n",
    "MODEL_TYPE_LOCALAI = 'LOCAL_LLM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVvFsppDfpel",
    "outputId": "5d25735d-1eae-4888-e643-8c296ce27aaa"
   },
   "outputs": [],
   "source": [
    "OPENAI_TOKEN = '<YOUR KEY>'\n",
    "OPEN_AI_MODEL = \"gpt-4o-mini\"\n",
    "openai_client = OpenAI(\n",
    "        api_key=OPENAI_TOKEN,\n",
    "    )\n",
    "\n",
    "# llama quantized\n",
    "LOCAL_MODEL_NAME = \"Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\"\n",
    "#or \"GPT4All-13B-snoozy.ggmlv3.q4_0.bin\"\n",
    "ollm_model = gpt4all.GPT4All(LOCAL_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yw30aS5vfpem"
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model_type):\n",
    "    if model_type == \"OPENAI\":\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=OPEN_AI_MODEL,\n",
    "            messages = messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    else:\n",
    "        with ollm_model.chat_session():\n",
    "            return ollm_model.generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MejaM6ov_jG"
   },
   "source": [
    "## Prompting Basics\n",
    "\n",
    "+ Be Clear and Provide Specific Instructions\n",
    "+ Allow Time to **Think**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "86fchLXjvkuz",
    "outputId": "3e2f07f5-7c33-469b-dd8f-68863f721fee"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output LOCAL_LLM"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proposed Transformer architecture, based solely on attention mechanisms without recurrence or convolutions, outperforms existing complex recurrent/convolutional neural network-based encoder-decoder models in quality while reducing training time and increasing parallelizability for machine translation tasks.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors introduce the Transformer, a novel network architecture for sequence transduction that relies exclusively on attention mechanisms, demonstrating superior performance in machine translation tasks compared to traditional recurrent and convolutional models, while also being more efficient in training time and parallelization. Key contributions include the elimination of recurrence and convolutions, and improved training efficiency and translation quality.\n"
     ]
    }
   ],
   "source": [
    "# Be Clear and Specific\n",
    "\n",
    "# Example: Clearly state which text to look at, provide delimiters\n",
    "text = \"\"\"\n",
    "The dominant sequence transduction models are based on complex recurrent or \n",
    "convolutional neural networks in an encoder-decoder configuration. The best \n",
    "performing models also connect the encoder and decoder through an attention \n",
    "mechanism. We propose a new simple network architecture, the Transformer, \n",
    "based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    "Experiments on two machine translation tasks show these models to be superior in quality \n",
    "while being more parallelizable and requiring significantly less time to train.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\\n",
    "into a single sentence. Identify key contributions.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_LOCALAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_LOCALAI))\n",
    "\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_OPENAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_OPENAI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output LOCAL_LLM"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Transformer Model for Machine Translation\n",
      "\n",
      "Summary: The proposed Transformer model, based solely on attention mechanisms without recurrence or convolutions, outperforms dominant sequence transduction models in quality while being more parallelizable and requiring less training time.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Summary of Transformer Model Proposal\n",
      "\n",
      "The Transformer is a novel network architecture that relies exclusively on attention mechanisms, eliminating the need for recurrent or convolutional layers, and demonstrates superior performance and efficiency in machine translation tasks.\n"
     ]
    }
   ],
   "source": [
    "# Be Clear and Specific\n",
    "text = \"\"\"\n",
    "The dominant sequence transduction models are based on complex recurrent or \n",
    "convolutional neural networks in an encoder-decoder configuration. The best \n",
    "performing models also connect the encoder and decoder through an attention \n",
    "mechanism. We propose a new simple network architecture, the Transformer, \n",
    "based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    "Experiments on two machine translation tasks show these models to be superior in quality \n",
    "while being more parallelizable and requiring significantly less time to train.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\\n",
    "into a single sentence. Provide response in markdown format\n",
    "with a title for the summary.\n",
    "```{text}```\n",
    "\n",
    "\"\"\"\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_LOCALAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_LOCALAI))\n",
    "\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_OPENAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_OPENAI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Wz4xbjPHwmI_",
    "outputId": "f084df1e-fa42-4e6c-e6c4-a6fd916e1efe"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output LOCAL_LLM"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make tea:\n",
      "1. Fill a cup with water.\n",
      "2. Add half a cup of milk.\n",
      "3. Put some sugar (to taste).\n",
      "4. Add tea leaves.\n",
      "5. Boil the mixture for about one minute.\n",
      "6. Serve in a tall glass.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point 1 - Have a cup full of water.  \n",
      "Point 2 - Have half a cup of milk.  \n",
      "Point 3 - Gather some sugar and tea leaves.  \n",
      "Point 4 - Start by boiling the water.  \n",
      "Point 5 - Once the water comes to a boil, add the milk.  \n",
      "Point 6 - Add the tea and let it boil for another minute.  \n",
      "Point 7 - Add sugar to taste.  \n",
      "Point 8 - Serve in a tall glass.  \n"
     ]
    }
   ],
   "source": [
    "# Be Clear and Specific, aka provide step by step instructions\n",
    "text = \"\"\"To make tea you first need to have a cup full of water,\n",
    "half cup milk, some sugar and tea leaves. Start by boiling water.\n",
    "Once it comes to a boil, add milk to it. Next step is to add tea and\n",
    "let it boil for another minute.\n",
    "Add sugar to taste. Serve in a tall glass\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Read the text delimited by triple single quotes.\n",
    "Check if it contains a sequence of instructions, \\\n",
    "re-write the instructions in the following format:\n",
    "\n",
    "Point 1 - ...\n",
    "Point 2 - â€¦\n",
    "â€¦\n",
    "Point N - â€¦\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\\n",
    "then apologize that you cannot rephrase such text.\n",
    "\n",
    "'''{text}'''\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_LOCALAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_LOCALAI))\n",
    "\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_OPENAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_OPENAI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AI0x-syTfpep",
    "outputId": "b0c506e6-45de-4b91-e247-a0aa722afb83"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snakes are elongated, legless reptiles belonging to the suborder Serpentes. They are part of the class Reptilia and are characterized by their unique body structure, which includes a long, flexible body, a lack of limbs, and a highly mobile jaw that allows them to consume prey much larger than their head. Snakes are found in a variety of habitats, including forests, deserts, grasslands, and aquatic environments, and they are distributed across every continent except Antarctica.\n",
      "\n",
      "Key features of snakes include:\n",
      "\n",
      "1. **Body Structure**: Snakes have a cylindrical body covered in scales, which can vary in texture and color. Their lack of limbs is a defining characteristic, and they move by contracting their muscles and using their scales to grip surfaces.\n",
      "\n",
      "2. **Diet**: Most snakes are carnivorous, feeding on a diet that can include rodents, birds, amphibians, fish, and even other reptiles. Some species are specialized feeders, while others are more generalist.\n",
      "\n",
      "3. **Locomotion**: Snakes move using various methods, including lateral undulation, rectilinear movement, sidewinding, and concertina movement, depending on their environment and the surface they are on.\n",
      "\n",
      "4. **Sensory Adaptations**: Snakes have a keen sense of smell, which they enhance by using their forked tongues to collect scent particles from the air. Many species also have specialized heat-sensing pits that allow them to detect warm-blooded prey.\n",
      "\n",
      "5. **Reproduction**: Snakes can be oviparous (laying eggs), viviparous (giving birth to live young), or ovoviviparous (producing eggs that hatch inside the body). The reproductive strategy varies among species.\n",
      "\n",
      "6. **Venom**: Some snakes possess venom, which they use to subdue prey and for defense. Venomous snakes have specialized fangs that allow them to inject venom into their prey. Notable examples include cobras, vipers, and rattlesnakes.\n",
      "\n",
      "7. **Ecological Role**: Snakes play important roles in ecosystems as both predators and prey. They help control populations of rodents and other small animals, and they are also a food source for larger predators.\n",
      "\n",
      "Overall, snakes are a diverse and fascinating group of reptiles with a wide range of adaptations that allow them to thrive in various environments.\n"
     ]
    }
   ],
   "source": [
    "# without instructions\n",
    "# openAI\n",
    "prompt= \"What are snakes?\"\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_OPENAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_OPENAI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IWcchnQKxgvX",
    "outputId": "e8603a5e-6e1e-421c-9f44-c31dfbaef702"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output LOCAL_LLM"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snakes are long, slithering reptiles.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snakes are long, legless reptiles that slither on the ground.\n"
     ]
    }
   ],
   "source": [
    "# Be Clear and Specific, aka provide examples\n",
    "prompt = f\"\"\"\n",
    "Your task is to answer in conversation style mentioned in triple back quotes.\n",
    "Keep answers very short similar to examples provided below.\n",
    "\n",
    "```\n",
    "<kid>: What are birds?\n",
    "<father>: birds are cute little creatures that can fly\n",
    "\n",
    "<kid>: What are whales?\n",
    "<father>: Whales are very big fish that roam the oceans\n",
    "```\n",
    "\n",
    "<kid>: What are snakes?\n",
    "<father>:\n",
    "\"\"\"\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_LOCALAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_LOCALAI))\n",
    "\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_OPENAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_OPENAI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7Lc-_zCuyElX",
    "outputId": "4d3ee791-a751-44de-a876-44d78b0132a4"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output LOCAL_LLM"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Unser letter Urlaub war in Deutschland. Wir besuchten Berlin und Hamburg.\n",
      "Translation to German: Our last vacation was in Germany. We visited Berlin and Hamburg.\n",
      "Cities mentioned: Berlin, Hamburg\n",
      "\n",
      "Python dictionary object:\n",
      "{\n",
      "    \"original_text\": \"Our last holiday was in Germany. We visited Berlin and Hamburg.\",\n",
      "    \"german_translation\": \"Unser letter Urlaub war in Deutschland. Wir besuchten Berlin und Hamburg.\",\n",
      "    \"num_cities\": 2,\n",
      "    \"city_names\": [\"Berlin\", \"Hamburg\"]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary:** The text describes a holiday in Germany where the cities Berlin and Hamburg were visited.\n",
      "\n",
      "1. **German Translation:** Unser letzter Urlaub war in Deutschland. Wir haben Berlin und Hamburg besucht.\n",
      "2. **Cities:** Berlin, Hamburg\n",
      "3. **Python Dictionary:**\n",
      "```python\n",
      "{\n",
      "    \"original_text\": \"Our last holiday was in Germany. We visited Berlin and Hamburg.\",\n",
      "    \"german_translation\": \"Unser letzter Urlaub war in Deutschland. Wir haben Berlin und Hamburg besucht.\",\n",
      "    \"num_cities\": 2,\n",
      "    \"city_names\": [\"Berlin\", \"Hamburg\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Allow for time to think (similar to step by step instructions)\n",
    "text = \"\"\"\n",
    "Our last holiday was in Germany. We visited Berlin and Hamburg.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple \\\n",
    "backticks briefly. Then follow the instructions :\n",
    "1 - Translate the summary to German.\n",
    "2 - List each city in the text.\n",
    "3 - Output a python dictionary object that contains the following \\\n",
    "keys: original_text, german_translation, num_cities, city_names.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_LOCALAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_LOCALAI))\n",
    "\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_OPENAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_OPENAI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LTg-YuwwyqgB",
    "outputId": "44613e8e-1133-4663-930d-186aa11dfa09"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> sample output LOCAL_LLM"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "```\n",
      "I went to the market and bought 10 apples.\n",
      "I gave 2 apples to the neighbor and 2 to the repairman.\n",
      "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
      "```\n",
      "User's solution:\n",
      "```\n",
      "1. I started with 10 apples.\n",
      "2. I gave away 2 apples to the neighbor and 2 to the repairman, so now I have 6 apples left.\n",
      "3. Then I bought 5 more apples, so now I have 11 apples.\n",
      "4. I then ate 1 apple, so I will have only 10 apples with me.\n",
      "```\n",
      "Actual Solution:\n",
      "```\n",
      "Step 1: Started with 10 apples.\n",
      "Step 2: Gave away a total of 4\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> sample output OPENAI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "1. I started with 10 apples.\n",
      "2. I gave away 2 apples to the neighbor and 2 to the repairman, so now I have 10 - 2 - 2 = 6 apples left.\n",
      "3. Then I bought 5 more apples, so now I have 6 + 5 = 11 apples.\n",
      "4. I then ate 1 apple, so I will have 11 - 1 = 10 apples left.\n",
      "```\n",
      "Is the user's solution the same as actual solution just calculated:\n",
      "```\n",
      "no\n",
      "```\n",
      "Final Answer:\n",
      "```\n",
      "incorrect\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Allow time to think, aka ask LLM to generate its own answer and then compare\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Determine if the user's solution delimited by triple back ticks\\\n",
    "is correct or not.\n",
    "To solve the problem the instructions are as follows:\n",
    "- Step 1: prepare your own solution to the problem.\n",
    "- Step 2: Compare your solution to the user's solution \\\n",
    "and evaluate if the user's solution is correct or not.\n",
    "Do not decide if the solution is correct until\n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "User's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the user's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Final Answer:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I went to the market and bought 10 apples.\n",
    "I gave 2 apples to the neighbor and 2 to the repairman.\n",
    "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "```\n",
    "User's solution:\n",
    "```\n",
    "1. I started with 10 apples.\n",
    "2. I gave away 2 apples to the neighbor and 2 to the repairman, so now I have 6 apples left.\n",
    "3. Then I bought 5 more apples, so now I have 11 apples.\n",
    "4. I then ate 1 apple, so I will have only 10 apples with me.\n",
    "```\n",
    "Actual Answer:\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_LOCALAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_LOCALAI))\n",
    "\n",
    "display(Markdown(f\"> sample output {MODEL_TYPE_OPENAI}\"))\n",
    "print(get_completion(prompt, MODEL_TYPE_OPENAI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRyn_X282b0L"
   },
   "source": [
    "## Types of Prompts\n",
    "\n",
    "<img src=\"./assets/pe_types.jpg\">\n",
    "\n",
    "### Zero-Shot Prompting\n",
    "Zero-shot or without any examples. Since LLMs are trained on huge amounts of data and instructions, they work pretty well without any specific examples (shots) for usual tasks such as summarization, sentiment classification, grammar checks, etc.\n",
    "\n",
    "_Sample Prompt_:\n",
    "```\n",
    "Classify the text as neutral, positive or negative.\n",
    "Text: The food at this restaurant is so bad.\n",
    "Sentiment:\n",
    "\n",
    "```\n",
    "\n",
    "### Few-Shot Prompting\n",
    "LLMs are good for basic instructions they are trained with but for complex requirements they need some hand-holding or some examples to better understand the instructions.\n",
    "\n",
    "_Sample Prompt_:\n",
    "```\n",
    "Superb drinks and amazing service! > Positive\n",
    "I don't understand why this place is so expensive, worst food ever. > Negative\n",
    "Totally worth it, tasty 100%. > Positive\n",
    "This place is such an utter waste of time. >\n",
    "```\n",
    "**Note**: We did not explicitly instruct our LLM to do sentiment classification, rather gave examples (few-shot) to help it understand\n",
    "\n",
    "\n",
    "### Chain of Thought (COT)\n",
    "Tasks which are more complex and require a bit of *reasoning* (careful there ðŸ˜‰ ) require special measures. Introduced by in a paper of similar title by [Wei et. al.](https://arxiv.org/abs/2201.11903) combines few-shot prompting with additional instructions for the LLM to think through while generating the response.\n",
    "\n",
    "_Sample Prompt_:\n",
    "<img src=\"./assets/cot_few_shot.png\">\n",
    "\n",
    "> Source: [Wei et. al.](https://arxiv.org/abs/2201.11903)\n",
    "\n",
    "#### COT Zero Shot âœ¨\n",
    "Extension of COT setup where instead of providing examples on how to solve a problem, we explicitly state ``Let's think step by step``. This was introduced by [Kojima et. al.](https://arxiv.org/abs/2205.11916)\n",
    "\n",
    "_Sample Prompt_:\n",
    "```\n",
    "I went to the market and bought 10 apples.\n",
    "I gave 2 apples to the neighbor and 2 to the repairman.\n",
    "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "Let's think step by step.\n",
    "```\n",
    "\n",
    "## Advanced Prompting Techniques\n",
    "Prompt Engineering or PE is an active area of research where new techniques\n",
    "are being explored every day. Some of these are:\n",
    "\n",
    "  - [Auto Chain of Thought](https://arxiv.org/abs/2210.03493)\n",
    "  - [Majority Vote or Self-Consistency](https://arxiv.org/abs/2203.11171)\n",
    "  - [Tree of Thoughts](https://arxiv.org/abs/2305.10601)\n",
    "  - Augmented Generation/Retrieval\n",
    "  - [Auto Prompt Engineering (APE)](https://arxiv.org/abs/2211.01910)\n",
    "  - [Multi-modal Prompting](https://arxiv.org/abs/2302.00923)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1av5flhfper"
   },
   "source": [
    "## LangChain ðŸ¦œðŸ”—\n",
    "- [LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework for developing LLM powered applications.\n",
    "- It provides capabilities to connect LLMs to a number of different sources of data\n",
    "- Provides interfaces for language models to interact with external environment (aka _Agentic_)\n",
    "- Provides for required levels of abstractions to designing end to end applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-IQGlLI24Ifm"
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=OPENAI_TOKEN,  # if you prefer to pass api key in directly instaed of using env vars\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a friendly chatbot assistant that responds in a conversational\n",
    "            manner to users questions. Keep the answers short, unless specifically\n",
    "            asked by the user to elaborate on something.\n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "\n",
    "            Answer:\n",
    "         \"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "      |\n",
    "    prompt_template\n",
    "      |\n",
    "    llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Australia is Canberra.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the capital of Australia?\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Conversation Buffer\n",
    "\n",
    "LangChain provides us with an easy to use interface to enable LLMs to refer to context/memory\n",
    "across multiple chains/calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BtgDe_qeCDZP"
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.callbacks import StdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a friendly chatbot assistant that responds to the instructions of the user. \n",
      "            You use conversation history as needed.\n",
      "            Conversation History: ``````\n",
      "            Instructions:Generate a title for the text delimited within angle braces <We went on Holiday to Germany. \n",
      "            We explored different castles and museums while visited the cities of Berlin and Hamburg>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghavbali/.pyenv/versions/3.11.9/envs/datahack/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/Users/raghavbali/.pyenv/versions/3.11.9/envs/datahack/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\"Discovering Germany: A Journey Through Castles and Culture in Berlin and Hamburg\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a friendly chatbot assistant that responds to the instructions of the user. \n",
    "            You use conversation history as needed.\n",
    "            Conversation History: ```{history}```\n",
    "            Instructions:{instructions}\"\"\"\n",
    "chain_with_history = LLMChain(\n",
    "    prompt=PromptTemplate.from_template(prompt),\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    chain_with_history.run(\n",
    "        {\n",
    "            \"instructions\": \"\"\"Generate a title for the text delimited within angle braces <We went on Holiday to Germany. \n",
    "            We explored different castles and museums while visited the cities of Berlin and Hamburg>\"\"\"\n",
    "        },\n",
    "        callbacks=[StdOutCallbackHandler()],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "svRWMXXq9FIU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a friendly chatbot assistant that responds to the instructions of the user. \n",
      "            You use conversation history as needed.\n",
      "            Conversation History: ```Human: Generate a title for the text delimited within angle braces <We went on Holiday to Germany. \n",
      "            We explored different castles and museums while visited the cities of Berlin and Hamburg>\n",
      "AI: \"Discovering Germany: A Journey Through Castles and Culture in Berlin and Hamburg\"```\n",
      "            Instructions:Translate AI Response response into German language\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\"Deutschland entdecken: Eine Reise durch SchlÃ¶sser und Kultur in Berlin und Hamburg\"\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chain_with_history.run(\n",
    "        {\n",
    "            \"instructions\": \"\"\"Translate AI Response response into German language\"\"\"\n",
    "        },\n",
    "        callbacks=[StdOutCallbackHandler()],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a friendly chatbot assistant that responds to the instructions of the user. \n",
      "            You use conversation history as needed.\n",
      "            Conversation History: ```Human: Generate a title for the text delimited within angle braces <We went on Holiday to Germany. \n",
      "            We explored different castles and museums while visited the cities of Berlin and Hamburg>\n",
      "AI: \"Discovering Germany: A Journey Through Castles and Culture in Berlin and Hamburg\"\n",
      "Human: Translate AI Response response into German language\n",
      "AI: \"Deutschland entdecken: Eine Reise durch SchlÃ¶sser und Kultur in Berlin und Hamburg\"```\n",
      "            Instructions:Translate the German title to English\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\"Discovering Germany: A Journey Through Castles and Culture in Berlin and Hamburg\"\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chain_with_history.run(\n",
    "        {\n",
    "            \"instructions\": \"\"\"Translate the German title to English\"\"\"\n",
    "        },\n",
    "        callbacks=[StdOutCallbackHandler()],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a friendly chatbot assistant that responds to the instructions of the user. \n",
      "            You use conversation history as needed.\n",
      "            Conversation History: ```Human: Translate AI Response response into German language\n",
      "AI: \"Deutschland entdecken: Eine Reise durch SchlÃ¶sser und Kultur in Berlin und Hamburg\"\n",
      "Human: Translate the German title to English\n",
      "AI: \"Discovering Germany: A Journey Through Castles and Culture in Berlin and Hamburg\"```\n",
      "            Instructions:Compare the original title you generated with german to english translated title, are they same?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yes, the original title I generated in German, \"Deutschland entdecken: Eine Reise durch SchlÃ¶sser und Kultur in Berlin und Hamburg,\" translates to \"Discovering Germany: A Journey Through Castles and Culture in Berlin and Hamburg\" in English. They are the same in meaning, just expressed in different languages.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chain_with_history.run(\n",
    "        {\n",
    "            \"instructions\": \"\"\"Compare the original title you generated with german to english translated title, are they same?\"\"\"\n",
    "        },\n",
    "        callbacks=[StdOutCallbackHandler()],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
