{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96259c76-dffd-4b6a-b4a9-29591e5e07d6",
   "metadata": {},
   "source": [
    "# Scaling Neural Nets and Efficient Training\n",
    "\n",
    "We have covered quite some ground in previous 2 modules and observed the steady increase in size and performance of the models. These gains come at huge cost, actual money and human labour apart from time researching and building these things. Can we estimate these costs and draw some insights about model sizes, datasets and comput requirements?"
   ]
  },
  {
   "attachments": {
    "08264d12-83d1-45ac-8664-90c3f5af5ad6.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAN4AAACMCAYAAAATIRdLAAAAAXNSR0IArs4c6QAAAIRlWElmTU0AKgAAAAgABQESAAMAAAABAAEAAAEaAAUAAAABAAAASgEbAAUAAAABAAAAUgEoAAMAAAABAAIAAIdpAAQAAAABAAAAWgAAAAAAAACQAAAAAQAAAJAAAAABAAOgAQADAAAAAQABAACgAgAEAAAAAQAAAN6gAwAEAAAAAQAAAIwAAAAAlHUM/AAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAK+xJREFUeAHtfQmAVMW19umeDWZhhn2RZQDZRBFREBFlAEWDxiXucXkQjZrE5Gk0Lv+vMuivcUlM4pbnc+UZjS9q3BVFYUAQBUE2AWUbNtm3GWaf7v6/r7qr53bP7Znume7py1gH7tytblXd0/Xdc+qcU1UuiS8VBLLT+3Hxzd7kZjjQIhyYG1ZKEc65xY1cccipAHlMC+TDY0OGA62VA9PxYkWBrVnv2BzgFaJkDThViYKCgpD9uHFG4CmGmD9HFAfmzq0TeEVFRcLNhgjCQpvrUV1qCvAKkDMBx71osE2bNi14zOuGDAdaEwcIPgKysLAw/LWaBMBYgFeAEkMAZ8AW/huY8x8DB6ZPJ9Yg7kJBOB6Xing9nlSIzHzcIOF8c+bM8RkyHPixcwDAU5jQ2MC+EFvcqBA5qQJYkCHDAcOBUA6EAZB4aZRSGklRiPtULwWZC1VLQ4YDhgOhHKCdw+VyaSNMAe6yC1eELSI1BLxCPKWQBtVSpkyZEjETc8Nw4MfOgaaAz45nhbho1MtQjcKcGQ40ygHaPzR2sCeObMlO4hUg5YtMbdRLcsGQ4UD0HMjPzw9XO+fi6eLwHOyAt4mJDOjCWWXODQei40CY2pmPp2aEP8lOoJUKcTKNFyBTuTNkOGA40EQOjB8/XhtcxiOLIms2busJjhXoKO0MGQ4YDjSPAxYvgMKVNTerqlmIGwVGxbSyxxwbDjSdA+zvMcysuLg4H7lQuyzCpsgq8eqhUicye8MBw4GmcSCS1CMKSQXY5vDA9O3IBUOGA/HjgF1fT0u8AhZj+nbxY7bJyXDAhgNBrVIDT10w4+dsWGUuGQ40kwMWdTOYE1XNAmxGzQyyxBwYDsSfA+HqJiVeAYuh08+Q4YDhQMtwgMBTZICnOWH2hgPx54AFXwXMPQi8+BdlcjQcMByIxAEDvEicMdcNB+LIAYvhchyzJfDUgeVGHIszWRkOGA7YccBIPDuumGuGAwnmgAFeghlssjccsOOAAZ4dV8w1w4EEc8AAL8EMNtkbDthxwADPjivmmuFAgjlggJdgBpvsDQfsOGCAZ8cVc81wIMEcMMBLMINN9oYDdhwwwLPjirlmOJBgDhjgJZjBJnvDATsOGODZccVcMxxIMAcM8BLMYJO94YAdBwzw7LhirhkOJJgDBngJZrDJ3nDAjgOpdhedfq3WK1Jd65M9pV4pWlspNR6Mb8L6ZJnpIuOHZEhuW7ekp7pwLfFv4vWJlFSJpOITtq9CZE+ZSHktVqnY75MOmSI924kcleOSzlmJr4uTSqiqqpLy8nJp27attGnTJm5V83g8cvjwYdm3b5907dpVMjMz1SIhcSughTI6YoDHpRz2l3nlX4sr5O2l5bJqS5VUVPmkd0e3eABELgyYinmxf/s/XsnLSpGR/TPkspMz5QwAMSvDJQRIvICIrAB8kdV7ROZv8ck320TmbvLJoUoRj8cnKQB8344Eo0tyMnzSFcAb3ccllwx1SSeAMV71aKE2EnMxnD35l7/8pWzdulXatWsnDz30kIwZM0YGDRoUc17WBzAjs1x99dWybNkyGT16tJx44oly0003SefOnSUjI8Oa1PHHjgcewbZsa628srBc3l1cCrB5MOmuv/ECa7JhJ8QdJRtPcJ27XdUeeWdRtby3+LC0b5ci1xXkyjnDMmRA11Rp15aJm05lNX6p9vpKn3wF0K3fhw9CuU/KqwlHUaDjfiOus0IEWZtUn3ywWuSBz9wy+RiXFI6nJGxePZi704hSbunSpcIZtfTEyJWVlXLttdfKk08+KTk5OdK9e3f1kYy17mvWrJFRo0YpacdnA1OjS21trVxzzTUybNiwWLNManqunTAFWz5XfM3Pz8ehc2hJcbW8sahcXigqlY+XHRYvxBaBxcbMPYl7Sjue6mu8rtNUVHtl/toK2bLfIxU1UP+y3EoV5f2mUBEk2xMLfTLre5+s2S1yAKCj6htOqj6Bi7zPunlQ/5U/+OR/lqF+EIun9GpiJcILc8C51+uVb7/9Vn7yk58oFTO8SiUlJUrlpJTi7xULEbznnnuubN68OfgYJRzz3LBhgxx//PGOBx6l9YwZM1j/YmwzHGtcmb2mSl6Ye1henl8qizdUSEoTa8qfmCroPIDvhbml8tRnh2XlNoitJtD/rvLJY/N98tFar2w/5EPf0v8hiCWrNNSlEiry//nQIxNf9MnM9UpQx5KF49JSunF75513VN+LFUxJSZG3335bjj32WFXfFStWyFdffaVAp6VhtC/y+eefy+rVUBkCxHJOP/10qPUe2blzp3z99df4KNt8/fQDDtw7UtX8fletPPrBIVmzvUr2lXia1SfSH1f+2N/tqJadhzyybX+tPHRZe+nXCSiIgqhE3lfkk4dne6Ud+mz4vZtFrFMmOL90i0eeXuhGo3HJ8O4u6ZHTrGyT9jABQKn0xRdfBOtw5513yvnnn6+kElVBGlp++OEH2bVrl+r30egSLb333nsqf6a//PLL5bzzzlP9x23btsn333+vgEdjC/t6Rwo1UY4k7vW+2FAtY6bjK7ahUvaVekLUx1hL1aDTz1H6lZR7ZObychlZuEOKvqvWtxrcPzAXkq7IC7BArWwEdMC36oNCGCqDjr/nVz971o1GmBU/eOVP83yyZLvfOFM/pfOv1NTUSGlpqezfv19V1u12y5VXXqmOL774YtW3S01NVdJwwYIFCozRvJWWjFhXXCVPS0sT5keiejl27Filvm7ZskUvAKnuHQl/HAW8tTtr5Yon9+DrVitVNV7Vb4uKiWGtm406HHTWfFwwetTAAHPlf+2RJVtgnrQhFC87D4vc85lP7p/lkVRXWCF4Rl1BWbkw2LC2mbCedu+QoraxfV0yqItL8nCvof5kKVwRS7d55dVlPpm9sX4ZNlVz3CUC7+DBg8rQwcr169dPjjrqKFVPSraJEydKXl6eHDhwQKg2Mq0mApb9n7Vr16rtk08+UVbLPXv2SEVFhZKS69atU8mzs7ODqivzHzx4sFJp6V6wSludt5P3jlA1D1VQP4e5/Ym9svtgtaRRFERJKWjVLmxsssqt4IdDo08TmAdLa+WsR3fJu7d0kTH90oLPUFrtLxd5GWB4/HNKuvpAZnntAaqj8lxyAtrYDaPcMqRTMAu4GdxSfEBk8TaffLvTpcBlB0C+KX2As9b5BN8bOfNol2TDH3kkEdXM3bt3B4HXu3dvSU+ve4mzzz5bCB6modVz/vz5av/ggw/KqlWrQl71pJNOkqOPPlpZMAcOHKjcBLSWkjp16iTMm0QQ8pxSkfe/+eYbdf1I+ZN04NG/tvaHWrnz9YOyZmuFtE1rHHRKnQNQ22WmyIl905XxxOtzSRlM+ks3VUEd9He0I+YUuEGjS1l5jVwOKfvuLZ1leK80pR7ugqRbCFfBjCU+SXMTYqFE0HbOcsmYfJdcAN/cmN4u6RjWZRmLa2PRRs4Z5JJF20ReWemS91Z4If1gkAnNzn+Gl1q3V+SS13zy3lXwSTpKF7GrcN01NnxKKJr2Sexr0bii6ZhjjlH9uo0bNyoAEnCbNm1SoNFp9H7JkiXC7fXXX1eWSqqtmugw131Dqp1ZWVkqD0pcq8VTp3fyPqnAI+h2HPTIF+uqZNE6P+gaUhHJSPaxenXJkHsvzJXJw9pIGSyEbdNd6vrBci9UPpG/zzksry4oUypriDXUpsUzwuXA4RqZ+tx+mXtXF6lCRMz8Ypc8tdArB8rqgy4d7SmnjUuuOMGtQDWie31paP3B2yNo44z+3FzyYh+3/N+ZPqmGiyOkXoEHdpT4YC31yp8XuOUPYxtWUa1lJPuYgCsrKwsCidLICpguXboowFB15GYlApRgat++vbpH0B46dEj1B5cvXx5irezWrVvwUT7HfiOJUo/g4z5WV0UwwxY+qPuctHDBLK4WOt3MFZXyIYwdbqiIjYGuyuOS287vIKse7C5Xjs6U9plu6dk+RTrCN9e1nVsGdUuVIdgevyJPVjzQXc48PgfqWwBtNqDTr0wVcPXWSpn8130ye12tzN3gkW9h9Kiy6f71yHXJece65YphLhnRo2HQ6fypOXO7boRLnr8EVkwUyI9OOLGKKZCIT33hk0113aDwZI47141dm/T1ua6ollL6nI500l133aUkHy2SlFjbt2+Xp59+Wn7961/L0KFDFRj1M9zn5uYGT1mGFdzBG0fIQdKARx9YVQ36NivLZfnmqgZBxwZZ63XL01M7y30XtEMfsHHu9sh1y5s3dZB7LuqgwNdgvxE/YhtIzWUby+X2t8rk7RXV+Hr61VV/SX4/FT8Uo3u7Zfp4l/Rrz15p7HTuAJGXLk1R/dJITx9AtM6v37VBZqQHknydAKBDWwOOrgOrqklrJ/uBTEcV8YILLlCqKVXOXr16qf4gn2W/sKCgQG6//XahkYXhYVaiO0ITDSosR0s5lqfL12mcvE+aqrkDKtUn31bJ+t01CAODTTBCKyYzqyG1nvxFR7n2NAQ6xkCUZHefky3dAcI/vX8Qai3VkbAMLAVT/Tuw57CkZKEvls2o5kBi9BkPQ6UdNyhdbhoDH1yd3SAss+hOzx8kMnuEW176yiMZNr8ArxWt88oLS93yC0hJpxNBR3WSoCLRoT1lyhT57rvvlIObRhWqgrREnnbaaUrS0TASiZhPx44d5b777lMxmfTdkWbOnCk9evRQ9+iYp4pbXV2tVM6ePXtGys6R121+9pap55Z9tfLs7BLZhT6epe2HFE6Q9OqUKpeOzgHomh7ef+3YTNm0uxZqbZl8Dyd6sCnbFOxCob4a6JgUx3APEHw1iIoY2D1N3rzSLW3jwDF+EB45yyULit2ycY/X1pCSneaTJxGaduEQF6ynIWxx3AlHCLDha0smndrcrEQwMVzsxRdfjFpFpEp62WWXKQvoI488oqTbjh07hBvD0yjtSB06dJBTTjnFWpzjj5Oiau4q8crq7TWyelu1lCpXgj2fMmDJGNgdUubMbPsEMVy965wcuWBEFnxtqfjh0fJtQKez8+Er6qGxwFuLHxfuhHS3PHdRquRA0sXL2kgAv3ixW7ogiDv4IdAVwJ7VK94LrWCD5aJDD+nAfuWVV4LhYnbVpKSidZMA1EYRu3R2166//nol5az3NOh4jWrnp59+GnTgW9M59TgO3+/YX43Bz0VrqwSRUmhg8MPhw+ULszbw9NSBGXL5KVnKeBJ7KaFPZKIPN/n4tspX9mxRiVTCshiRKPWgxngrMNYvLVN+MSpNTu5pB4+IOUR1Y1gXkRtHifxxtl+pDS+B3/Pnv/bKZTDmOJVoFPnggw/kz3/+s/KnsZ7sb51xxhkydepUGTFihOrfUSWkVbIp/bD+/fsrxzv7ilRX6YqYNWuWilahWks1lhZQqrEEIEdAOJ2SArxlW6tlFSReUHoQfJRCCoAABPYeoHIUxtSNOTojLjykBBncPRVSr40sRFjaovXlStLwui0B+ZUlFTJkYFspnBSFNcc2k4YvsuyR6JpkQKJyYC9HX1iJLPmy2CdzN6N/ifF8TiNKnaeeekreeuut4KBX9r3efPNNFb1irS+BZzW4WO9FczxkyJBgsvz8fBkwYICcc845wjhOBk0z8oWB1JdeeqkCZHPKChaUwIMW/5SWVMJUjv7WzoM2tnq2LVo4sI3o30aO7Zku3WAYiRdR6nFM3h1QO6nu0EoZmeDegJpZcaBEha9FTte8O12yXXJaP45QZ11C60N2cMDtEwvxIcKtQJemeQXG8Wk6xDnigCFfVCFvvPFGWbx4cT3QsUj2/+IJBvYpKU1/85vfyEUXXRTsNzIqhtecTvFr1VG+6efrqmXLPq9qUErtCBM5bFzcLh2dJUN6pMUUPhZNFbIz3DJuYLo8fnV78bngPQxt63VZoNXTbbF5d7Xc/e8SW79bXeKmH3WBzeiXI13SGy4qpW6HVSg9BRJvPZz6m+lSaXo5iXjyhhtuUJZLjjLnQNQLL7ww5v5bc+pF/yDdET//+c9DXA+vvvqqivdsTt6JfrZFgUdN6tPVVbITow6UngddSvXx4N9xYeNFtruUNLcUDMqQ7nnxrx4FagaiVc4c1lbyOiLkqBEOE3xvLCqTJZujG8nQSHb1budAk2ZoGUPQGPamGGABH9XxGiDu3bW0rtZ7PGkXGNb12WefKYMG1b5bb71VGFuZDGIf8JlnnpE+ffqo4hlFw2BsJ1P8W3YDb0un+debEclegebODgylndr4EI/dUuWBRBqSLb0Q5U9rYrwJgxJkDwKgp76OIGlXlt/CGV4IqqKJ1SvDdBO3vV6iLjU2LEg/F+2ewGIYGqNvMiGNFVmARzYx6mXlDowDdBDw3njjjUBVfdK3b18l7Rj2lQyiH5EbJTCJETQ0vujY0WTUqbEy49+yGyiRTvPN+zwIZkYiDTg2crXBkkjzJkTSbyZmShsES1M6xZsOYkKiV5ZDddvoxViuVElhGBJ+KEW6LjjR3wReVyPYV5fLtPcw5gxRJYmggbBwDojgU2ZdStE3Bv5VXy8R5ceSJw0l7MuR2ODp7Oae/bxkEocfMXCatHLlSgU8q9shmXULL7tFrZqz1larCBAvGhBEHjYAjWBT0g6NCteP6ZWBmcHSpRrzo8SbDgPwG/b55J/f+CQjxavUTBcCen1VlXAdMHiXyPODTh0E/vBqZjoseLNK5BjEgl5yUtvgd8OarjnHtG4WY4Kkldvrl898yZsth/zSsR3U02QSx9BxJDmJznMGRTdGNPnbAZNj8zh9A0PKSPQJ0ufXF1KUfUe7Z5hu7969algQjzVx9ALjOalqMryMc7LQuR6r31Dnl8h9AmRK5Oq+t5KIQz8OKiXVypBNATFFzjkuQx2lUr+KM32/xyfvr8EIgIN+UKsS0PFMyc2DpIW+B6J0sSPWp7rGIx+sqJB9kHo0/8eTBndyyTEYOOvR0pd7fYyCKhBI83mxTzhkKdlEEFHqkThnpt28mRxhMGnSJGXN/Otf/6oARenDaSLoc6MqOHLkSBUITVfBCSecoLYHHnhAHn30UXnhhReEo9WttH79ehU8zXAzWi85sFYHZjMdPwK6LqwjgcfynEgtJvHoRliOkdYMlPU3Wf7VrRxqJo7T0fhPG+if/DTeuGPf7ostIm9/CyNFiBsB9UCdUvDV9pZApPCDYOlj6R9N1RSo3LSnRmZhIqYJsIx2y/WDVadpzp6DXxkaVoW6pXIMoP4CsC44xsgljFAXOQG+4QGYszOZpAxigfqx4Vsbv64Xp34guEg0vOTn56tR6PS3Pfvss3rGLQVM9g0pNZkP86ZTnE55Tu1AAHPuFhLz1JMe/eMf/1DDiTjpkR79wD6dBpoOyFaWc/W0s/6glbUMrdiOGaHgu6tjBJuy3mjWxzQJmIh2QGd/lQK/a9wqt3gbgncB/N2Hvbb9JHd2jmTn4OsdYSCu6ivAr7d5b60a77cZ0wXG27xPQ0ubtFTwwtKPDDCiEqr3yp0YGR86nC1u/IklI/ajqAaSqNYxZMtKvMZJZzXxN6cqyWt33HGHmhOTedx///1q0iJOfMsR6gyq/uMf/6j8cJwAl6PTf/azn8nf/vY3lZUV4BxCxHhQqwGFElDXhXGeTlUz+TItBryl29GnwsdbNSma6lgytwD2PABet7wU6YxZlxNBzy3yyiKMKrc0aX8xKI5ujiHd3PLhzR2le/s0GHVs6oA0FNWHMFnSmm1VsgJTBO7CDGjxJIK+I7pLIQI5UABBvvewTyoCE+fGs9xY8yJodCQJh+boOVQ0MHh/8uTJwWzPPPNMNch19uzZsnDhQhWzybCvu+++OziqgeBkf45S7le/+pU8//zzylLKPJmOwHz44YeD/bXjjjtO5WPtvzFwmuAj0cVBgw8lnxOpxWrFgaUpMHr5FOjQsMmQwObDHrIQAdEpmD7PptE3k3Pzt4gsx0SyXNcAnkN/bqgHfYeY6xmTy6bI7051ybAeqXJSfrp0zAlVIUM0T55g+9ficpnNeFOeNrN++vFsGE0GdnHDX+efQyaobiIBy6mGuhlvKavLjmVPkJx11lnqEUocSh9aESnpNNGvRgnHftr7778vnIrv448/VnGUt912W8hUfJx/k1KPjm8aRbRj/He/+5389Kc/VVKMY/cmTJigRiXQf8joFM5YzbRavWSZGvzjxo0LglTXyUn7FgEeJ/GhRS4FHTf25UIIp2y4tZB4x/bgYMaQu80+OQQbwHVveqGiWaQdyiAAWS5HmQ/uhlHlg/xF/cfYLBnLfmagHn4V0wItHNbAkz3/u0pZgCkrOLoiXr49WitP6MFpLAJMsb69qgK1BktdrPdb+Jhj5Gh9pDGDxpLf//73UlxcHKwFwckp+KgyUupQLaQlkoC1TuHAkDNGvHASI/bhCE79jvn5+UE1k/GYVFfppCcAmbeWZpRyHDZEQJJYJ/YL9f1gpRx00CLAo0VuH+Yv4Yo+ClkBSaclHiWPG067446Kr62HUuKhuT4MgMXaBqhDAEuoQuC1cd+Hj8FD+HhnBITc2P7pMqpvBuZxgY+P0jlCO09ze2X9rmp5d0UVXCT1FNgm/cTtMJfLcZhWhAEs9QjX2Ad0gsRj3dioKWG4EAldC1QjCSI7IpBoZWS/i+PmdP+QaU8++WQVaM1jTlTLKR8IWhJVz759+0p+fr6SelqNVDctfz766CPhBLqaGDxNldXJ1CLA44IeJRX+Aa/kafjGdQTSECbWL87Wum93++SlxV7EXELa2QCII4OuH4mQrV6hP9GxPdPk3OEwTSO0zE/1H6b78ZviGnnsY/i0EBgQD8qEKp7f3jLDmEW6sSZgkfqAlMfZldHUulPVo/Nck9XQoa9xTyBx40RH7NuFT3jElX9InCeTBhFNVBvZf+QMZuzL0V1gR1rV1PcoEZ1OLQI8TjhU5WM8JpqP2lCsReqlIDQkF0GLXbLjW52bP4DVDW6McNKqTBcYcm4Zo8FVl+rEPmlyw7hMzCbmD6L2wePv89U3pHAawQNltbJgI+doqXu+qUec8qErjCt0JwQ/FJaMWVOuxXegon6dm1pmc56jAYR9M/azqD7SF2dHBB2DmSklGWqmZ5zWaalm0qDCWaKtUzgQcJz+gX1HqqzWyY70s9wXFBQodVVf47Jg1v6mvu6kfXxbeoQ3q8QXmsNblKgLKnx1ialC5WZ44zqR6/1QMT/f6MUClXXlBI+o+gBHl2CmMI4KCCdKuh6wsJ4+mB33GnTYgSoCywIC/czhSkzBPhOzISMUrrlEAZsLf14mAO8vLzRHagql1S61Dl/oneSc0TFOwwpBxSkaOO9KJGKfi9ZNru7D/pgVfHSIc3YxOtSpjtI5vwPTO9CR/pe//EVJy3vvvTdS1spg8/e//z3oPOccL/PmzYuY3gk37Jpl3OtVDfs4IzL8Kkf97NPQ4vIwVR8BGA9ajmimZzBfCectsSP2kzpgfbr/jDBNBxs4l/O65axszNMCVXJ/lRrRAKHt/3ZYMuWYvu37qmXKSwdkzq2dlEC33I7pkLGpHDmRiajpKgZm2gg2+vO44qztzZhKa15ijk6g2sgxdhwVMGXKlHohXNYSaAyhcYUGkC+//FKt5kqwXnHFFUpaMh/2A7lOAqUonejMn23m5ptvllNPPdWaXcgxx/oR9Jy97LXXXlMj4TmlO2ew5vNOpBYBXhUMG1xbQNs0QhgBvrCPl4Xp0ONB9IHRoFIGgweyrUfUdAn0ByeJdPPH09ZLwwsEwOBuafLgJR1l6n/vUJY29uuU5SMsX+b55feV8qdZZXL7pAYytS2p7iKz5TAkbehRd8IaDocGMQon2aQlChs2Gz3H44X3tax1pG+PhhUaZBh18tJLL6l5Wl7BXC1UQdmfI/ioarIPSAsmp3IgOMOn+bPmy2MNLoaoEXgkThXP+lj9fOqGQ/60CPDYWJTsYcsKabT+C24gMr2ZQxGYP3N7EQHQb6/yr3dgx2NaDk/r51bLItvd19d0e78Kc768tyxX3v261D+de0j9dWr/qq/PzClFAHUb6YshTU0i5M1ygXm19/8JzYnarjWwJfRuy51RZSQRLDpki8eRiODgLNE0njCYmWFkL7/8svLxMeiahhk6vCmlmIbrJ9BiylVgoyW6GlgXApfB1tqnF+3zLZmuRYCnGrFqsPzDDa3Heo4EKk0z3pzrj6+Ainnnh5jSHVZMlX1YfrzKlX2uGo4hR3YJwtLr0zsm58rsVeUNShrWfyciWf7fh2Xy/FXt1IcmhiJ0Uf69Eq04tGEKL1HCJpNonNLRKpRWkUYQhNeR4KMEovWSxIBoblbikss63EsHPFvvN3RMNwWfIfAYOqaNaA09k6x7cepVNVx9rk9APx0jVPCNRIPixg4TTvFjUD2saIaJnM//UCry1JfoS6IDF6lhdkd44QjMFjY8xkmoRvRJl/NHtZNyTGobQmEASAXgZ3xxWF77pkrYF4uVKM1ox1EjzW1Ax/yohmZEiCeNtbympqckYX8sEcRQNKqbdB0Q1LEQJa5+hvX70QOP/qlUN4Srj4BDa/X/xw4uBvyrRYjU3jKuMdc02oTwvDkbffLFJvvFQJgrwTm0K5YCO1YkC5bDWOlPl+ZJj44ZGBQbQBvQzX9B4jneLc3llVtfK5F565UFJHg7mgP1AQJgSyG9I1E2JmxK9ng8NnD22Uhs3E5R6bhqkf4gUOXUfb9IvEzm9dg+KU2sKZfeqnNGE3dssHUbgbcTwNtrszpPY0VSrnyJ4Oc3V3ItBp7ZUxZiQLnICOfH5FQLsRIXmHxmanvp2QFfEZB6BwvutJhNBzBLEUj9r68rhUOhYjGEMHxtByQ3A7GtWeu6UiLmIZotfEkwfb8l99p1QAOGHpuXbAnDAbXaOU8XhZZ+LcmXaMtqEeBR4rWllGFrYolK6ml1ExIPDaoEg0tX7rZrbg2/yvKdWGcA0m7NLovTOewRwnHiAJecBNBRWkRSRcMeq3c6cTA6/5gkiWpzCDIsGfIry9PlW2vk38ux5jpmzY78OQgt4gCG/HyD96lmrGYYMQ/yKQdrsBN8ySZOrUeitKNPjs5uvYBksurGwbFa4lFl/dEDjzNpcapyGjdU9EpA3fQDkI2MSy655P210TZR/09L1YzrCyzaSgd95GcZ7U+DCgOQWXRTiSsOnTm0reRjfb7gxETMTwOPjj5u+P8Dwsge/aRcVmLRTYbMRUO7Mezn9eUIcYtQR4a4UaJGuB1NEXFLw5miaYWksYSzSf/zn/8McYrHraAoMqLE5cbRDSSqwvT7NWRljSLbhCZBE0k8cXT10Z3RzyLwFOjQdILg8x9mwDDx6To4o/2TeUVVqQfmiZq0iAs62gGKzZ3O5p+fiLXzOsJJHwdJkY9FVK4ak6U+JEp6a9ApOPC9+D5ugM0Lx3qNfLy6WpZsbdzxRqPKzlKfzNsAs3pq/fch2PgFb6bXJSq+RpOIJn+OJqDKSUc3/XPcJ4M45OgPf/hDcHQEQ8smTJiQjKpEXWaLAI+1GZvvH/oTqUA2qG0HfPLqCr90YEhXJGLkyf1zRR4r8iB8CtNG+B+pl5yNeXBXtzx2tks6IL42iJF6KaO/kN8xRaaMaSuDsHpQdpuAsYiPU9KRAl8AAoWG29eWVGE5Lv+4PXU/wp/NGEHxPVYO8uDF1YAl9VJ1L0bHeifYM+yW9YqQZcIvT58+Xfr27avmNuFAVTqtk0GMdmHImaZrrrlG1UufO3EfCQdxr+uko9H4cxoujg3u/s988vhXPtmJ2QQ4DSBN6wSaJqqXv5+J5YoBulT4uyIBlKCjBfLhszFfJfqYWX6biM6myXvEc0s2DDWTj8+U4X0gQokw/x+1C1rSlET3r4mwAGs1nP74YQzErQOSrgDfcV+5T57FCPkXv/ZAPdJ3sLckp4FqdG+M0k/QCH1LqVEfcjwefWc0qtDIQh9cS5I25lDV1VEztLbyg+B0ahEHOpnA8KxJA0TeX+VvT6q9hnGHEikNYRn3fuSS1bvcMr4fpvvrjNhFgAajimTmOpH//tIrnJ+zsbluS9GgbxzjljP7+wuxU0XDio/6lH7Jq0e3VWsvcIZpjgy3FdB4H5bLKSJ2lnpl5GM+uW5MG7nwuFTpjtWIKzBqY9Fmj7yJSJsl27CeBFTmSFKZavKV6Kf2rxs1E3V9E5WQo8XpqKYKrKdtSFRZdvnqjxxHRbB89vM4KoFLPHMxFSdTiwGPTLh5DJZe/o4rvNL3Y88WNlRKsv9d6pE3lqOjjJbIcWicE9MLMcbjkFhGm2wYXzxxUKo8cY7NzThcQhWVjDu+V7pcfnK2vLUE846UIxo13OHLhEyLPbfSMo88NLNCHv7EhRWCIN+hnnohLfjlZryqWrdPPRAQdVQ3+SAoAx+f0/r43QnqggP+PPfcc2pKB84QxqWxuP5dMogxnffcc4/aWD6HHnEiJevYvmTUq6EyG9b9GnqyCfeGQnqd2IsrnAZaZAN5KCMC2p0HuiWjQAhG+t8C7bCBJ0V6t8cyxz9rMElcbo7olSq3nZEpfTqn+ecKteYawlkYlfAB8cG5ng7DCT8eKt4SfzjrCzBX773UfeTnwxeKU7f36sB41vj0U63VbM4xl+fiEBxGmnB5rGRN4c5yGfupy2fsJ2coczKFNI9EV5TDfm4a7Zb+sDByOr94E6UoJ8t94gK3dGt8cuNmF0+1kGsePHhhjuQgHCbowCdXrV8I9ar8Q8RxH5BoEWvgv0/w0aneFuPz7jydgdgRH2jxG4zV5HR8VO0Y/MyxdpEGqia6cnQbMFKF4/lIVDl1EHeiy25q/i0KPFZyQl9RRoIeeTSlxI9o9csFCF6+3CUT8uOXb2M50co4qneqfPTbHMnLSfNPzUdzZpAANO3f05bP4L0GDlwc9Q6J73XLo5MRX4oJmZziSmCtOc8KHeZ0oNPAwpEBsQY1N/D2TbqlVyuioYVRLNr40qTMEvyQtYUkuCh/9mw8Fwx1yeXHI1Jd9WuaXyzz7NfJ30AnD6BEaTmiYCPoj+uWIu/8qp10ykuziTllnSz1iuqL41/E5ZZxKXIlRspTulpyaLkXjFASp0fXRGmTl5eXdIe1tU9npn7Qv45lPwjz45w72CU3Qu3sA8ln1cosyaI8dMm4fm757SkuocsimTQSku/f1+dK787p0kabXflp45TsVrKrZlgSho2dNyxV7plol9iaWXKOreFYlDDcki1hrMHa2uKZHO40XmqLSzxWierZQIDv2pNcMmWkW8bku5UBofHq1qWgn47B1dec5JbrRgF0kHQ5TRh1UJdjfI6G93DLgv/MkRvGZkrX3DRJhyUlpD8LHNWzfrJo6JW02nJzpaTKI+dlyIyL0XdxUL/OyiHr7GI0ZhRjTs1kx2py8CuJoGN/08ngS9rP2hbm8V4YH3cx1M6hWCXnaGwvL8U0gHAmMyCEepX1C6osgQAbJQEnvx16lEvumeCSIQBwD/jEGJbmFOIo96mj02V4zxSZt7FWlmH6+q82eZVllh4HqwxjcADdCHmZWJ+9c4p0a58qd49348NkTeWUN6urByc4ohWR/TpKO06fTiMLB7G2NLGdUNpxMlwSz7lKrJMpacAjU6hicjo7OocL+rrk3gKXfLVdZN4mWPM4FycGnuphNWv2iPSBSX10L5+MQ9oTuvqfp+Rj/8dpRJfGUbluOQbT0m/Y65NNWPfdH4uJhVMwEoOB25zXZeLRbjUqvidU7ny836QBSVFCYmYfLYmcxJazSFO69OvXL+K8lzFn3oQHWB+6NzhXCwO3I0012ISsE/JIUoGn30jHH+ZiFMOkfv5tw34XJm/FfJwwp9PJPApDevyk9/4zJ4KONWMt6T4ZCKMPV3qt9qTIAUjz+xA3qgnfDPkOk+52wPrnHK7kJHeBrmND++HDhwu3ZJNWKTmZ0pFCjgCeHbNCQ6PqGqtdWqdfY+0ZbRMeZ8nrg6FiG/rxceDI0Gt+fL+LeeNWzgEDvFb+A5vXcyYHDPCc+buYWrVyDhjgtfIf2LyeMzlggOfM38XUqpVzwACvlf/A5vWcyQEDPGf+LqZWrZwDBnit/Ac2r+dMDhjgOfN3MbVq5RwwwGvlP7B5PWdywADPmb+LqVUr54ABXiv/gc3rOZMDBnjO/F1MrVo5BwzwWvkPbF7PmRwwwHPm72Jq1co5YIDXyn9g83rO5IABnjN/F1OrVs4BA7xW/gOb13MmBwzwnPm7mFq1cg4Y4LXyH9i8njM4MHfuXF0RdUDgqQPLDZ3A7A0HDAcSxAEj8RLEWJOt4UBDHCDwipigqEjteGjIcMBwIM4cCMeXkXhxZrDJznDAjgMW4BXxvpZ4uF4klpt2z5prhgOGA03gQBiuiphFiMSbPn16E7I1jxgOGA40xAGL4TIIMA284IWGMjD3DAcMB2LnQGFhoX6oSB9o4KkLRt3UbDF7w4H4cCBMiyzSuWrg8VxJvbCEOp3ZGw4YDjSPAyFapSssrzk4L5gzZ44UFBSE3TKnhgOGA7FwgELMomaGYC3kBJkWYJtD0BF8hgwHDAeazgG9bh9yoLQrtObERY+tVIyTAqxnnc+HjNSzssYcGw5EzwFKO4sbYXz4k+HA4/3N2KbwIQO+cHaZc8OBxjkQpmISdMXhT9kBj4lc2AoIPkq9/Px8nBoyHDAcaIwDYaCbjvQv2T1jBzymK8KmwDdjxgwj+cgRQ4YDjXCAgmrq1Kk6FUFXqE/C95GAx3RF2BT4jNpJdhgyHIjMAUq6aEHHXBoCHu8XYTPgIycMGQ5E4ICNelkYIWnMl5mRT2/wTfgMGQ782DkAl5sPNpAgLoAP4iQqojSLlgqQMOjc047BadOmRfu8SWc40Co4wK4XpRz3AeIB+3TcR0WxAE9nWIiDELQRhOPGjVMWUJ3I7A0HWhMHNNj4Ts0BnOZJU4Cnny3EQQgAeYPuB26aCEhDhgNHGgf0UB4NMr23vAclXKHlPKbD5gBPF1SAA271QIhrhgwHWgsHigIvQsDp48Cl2HfxAJ611AKccLOSEXlWbpjjI4UDcwMVLQrbx6X+/x/jIocmzqY8/AAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "d087106d-f174-4fc0-bc53-042842996eff",
   "metadata": {},
   "source": [
    "## Estimating Compute Costs\n",
    "> Back of the Envelope Calculations : A quick way to get rough estimates\n",
    "\n",
    "\n",
    "**[LLaMA 3.1](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)** from Meta.AI launched very recently. The model is available in 8B, 70B and 405B sizes and is outperforming a number of existing LLMs on various benchmarks. \n",
    "\n",
    "![image.png](attachment:08264d12-83d1-45ac-8664-90c3f5af5ad6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de8150-f71a-4478-89b6-075cf10a602b",
   "metadata": {},
   "source": [
    "## But how much does it cost to train such model(s)?\n",
    "<img src=\"../assets/cost_tweet.png\">\n",
    "\n",
    "> Source: https://x.com/deedydas/status/1629312480165109760\n",
    "\n",
    "__Assumptions__\n",
    "For the sake of our understanding, we will make the following assumptions:\n",
    "- Ignore costs associated with preparing datasets\n",
    "- Ignore costs associated with training restarts, infra-failures, etc.\n",
    "- Cost of forward and backward pass is set to 1\n",
    "- Assume a very simplified view of overhead associated with multi-GPU/multi-node clusters by setting a standard efficiency ratio (ex: 0.25 efficiency in terms of TFLOPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e81bd-68c3-4d3f-b04b-2fb47d176b16",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "- Model Size : 405 **B**illion\n",
    "- Training Dataset : 15 **T**rillion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baea2c80-41aa-420a-a9f1-a84b75ff0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and dataset size\n",
    "model_name = 'LLaMA3.1'\n",
    "model_size = 405e9\n",
    "dataset_size = 15e12 #TODO : 15Trillion Tokens. Hint use scientific notation\n",
    "forward_backward_pass_ops = 1 # better estimate from table 1 @ Kaplan et. al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf57b284-9971-4dbe-b3b5-42346a8d0cea",
   "metadata": {},
   "source": [
    "### Compute Required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce98c19-04c0-429b-b198-81afad05316f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will need approximately \u001b[1m6.075e+24\u001b[0m FLOPs to train \u001b[1mLLaMA3.1\u001b[0m\n",
      "\t,where FLOPs is Floating Point Operations Per Second\n"
     ]
    }
   ],
   "source": [
    "APPROX_COMPUTE_REQUIRED = model_size * dataset_size * forward_backward_pass_ops\n",
    "print(f\"We will need approximately \\033[1m{APPROX_COMPUTE_REQUIRED}\\033[0m FLOPs to train \\033[1m{model_name}\\033[0m\")\n",
    "print(\"\\t,where FLOPs is Floating Point Operations Per Second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97faa229-cc24-4635-8ade-ade15bd51eb4",
   "metadata": {},
   "source": [
    "### GPU Performance and Compute Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda26c00-0ff2-4ff5-b337-122abacf2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost source: https://fullstackdeeplearning.com/cloud-gpus/\n",
    "gpu_details = {\n",
    "    't4':{\n",
    "        'flops':0.081e14, #colab free\n",
    "        'cost':0.21, #usd per hour\n",
    "        'ram':16 #gb\n",
    "    },\n",
    "    'v100':{\n",
    "        'flops':0.164e14, #standard nvidia\n",
    "        'cost':0.84, #usd per hour\n",
    "        'ram':32 #gb\n",
    "        \n",
    "    },\n",
    "    'a100':{\n",
    "        'flops':3.12e14, #standard nvidia\n",
    "        'cost':1.1, #usd per hour\n",
    "        'ram':80 #gb\n",
    "    },\n",
    "}\n",
    "hour_constant = 60*60 # number of seconds in an hour\n",
    "gpu_efficiency = 0.5 #50% efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db43d82-5fc3-4bce-8e51-5b76dd49d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will need approximately \u001b[1m1.08E+07\u001b[0m GPU hours to train \u001b[1mLLaMA3.1\u001b[0m on a \u001b[1ma100\u001b[0m GPU\n"
     ]
    }
   ],
   "source": [
    "gpu = 'a100' #TODO: Select one of the GPUs, ex: a100\n",
    "COMPUTE_TIME = APPROX_COMPUTE_REQUIRED/(gpu_details.get(gpu).get('flops')*hour_constant*gpu_efficiency)\n",
    "print(f\"We will need approximately \\033[1m{COMPUTE_TIME:.2E}\\033[0m GPU hours to train \\033[1m{model_name}\\033[0m on a \\033[1m{gpu}\\033[0m GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313aa189-1ec4-4885-9300-022284d39480",
   "metadata": {},
   "source": [
    "### Cost of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7383ba9-a742-4a26-a361-046770020450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will need approximately spend \u001b[1m$11,899,038.46\u001b[0m to train \u001b[1mLLaMA3.1\u001b[0m on a \u001b[1ma100\u001b[0m GPU\n"
     ]
    }
   ],
   "source": [
    "TRAINING_COST = COMPUTE_TIME*gpu_details.get(gpu).get('cost')\n",
    "print(f\"We will need approximately spend \\033[1m${TRAINING_COST:,.2f}\\033[0m to train \\033[1m{model_name}\\033[0m on a \\033[1m{gpu}\\033[0m GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02f0be-2c00-440c-9303-597c68c16893",
   "metadata": {},
   "source": [
    "## Big but How Big?\n",
    "\n",
    "The latest and the greatest seem to be a thing only the _GPU-rich_ can afford to play with. The exponential increase in the size of models along with their training datasets (we saw GPT vs GPT2 vs GPT3.5 in the previous module) indicates scale is our best friend. \n",
    "\n",
    "Work by Kaplan et. al. in the work titled **[Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361)** presents some interesting takeaways. \n",
    "We will use the notation from paper as:\n",
    "- **$N$**: Model parameters excluding embeddings\n",
    "- **$D$**: Size of the dataset\n",
    "- **$C$**: Compute used for training the model\n",
    "\n",
    "_Scale is a function of $N$, $D$ and $C$_\n",
    "\n",
    "\n",
    "Let's look at some of the insights from the paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb1e79-23b5-42b4-81c2-a77486a9ac19",
   "metadata": {},
   "source": [
    "1. Performance depends **strongly on scale** and weakly on model shape\n",
    "2. Performance improves predictably as long as we **scale up** **$N$** and **$D$** : \n",
    "_Every time we increase model size 8x, we only need to increase the dataset by roughly 5x_\n",
    "3. Large Models are more **sample efficient** than small models reaching same level of performance with fewer steps and fewer data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da49860-5be3-4cd6-bac7-fcfde7403193",
   "metadata": {},
   "source": [
    "<img src=\"../assets/scaling_laws.png\">\n",
    "\n",
    "> Source: [Kaplan et. al.](https://arxiv.org/pdf/2001.08361)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3aab8-83db-4bcd-ae71-84f723fd7194",
   "metadata": {},
   "source": [
    "## So Should We Just Keep Growing?\n",
    "\n",
    "**TL;DR**: Probably not! \n",
    "\n",
    "**Long Answer**: In their work titled [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556) Hoffman et. al. build upon the previous works to showcase that current(_2022_) set of models are **significantly under trained** or the current set of LLMs are far too large for their compute budgets and datasets!\n",
    "\n",
    "They present a 70B parameter model titled **Chincilla** which was:\n",
    "- 4x smaller than 280B parameter Gopher\n",
    "- trained on 4x more data than Gopher, 1.3T tokens vs 300B tokens\n",
    "\n",
    "and yet **outperformed** Gopher on every task they evaluated!\n",
    "\n",
    "<img src=\"../assets/chinchilla.png\">\n",
    "\n",
    "> Source: [Hoffman et. al.](https://arxiv.org/pdf/2203.15556)\n",
    "> Fine-print: Though undertrained, LLMs increasingly show performance improvement with increasing dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fcc526-f317-404d-bbaa-0bbd5204a82e",
   "metadata": {},
   "source": [
    "## Ok, So I have a lot of Compute, What's the Problem?\n",
    "\n",
    "The scaling laws are all good for BigTech, but you could say that most companies have a lot of compute available. Where is the problem? Let us understand this with a simple example walk through\n",
    "\n",
    "Assumptions/Setup:\n",
    "- System RAM (CPU): 32GB\n",
    "- GPU RAM : 32 GB\n",
    "- Model Size : 20B\n",
    "- Parameter Size: 2bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18976f94-44ff-41ae-923e-5f851e15ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import humanbytes, memory_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0ea1172-6d32-417d-832d-1b5aa51c80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU_RAM = 32e9 # 32GB\n",
    "GPU_RAM = 32e9 #32GB\n",
    "model_size = 20e9 #20B\n",
    "param_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a845b652-3eff-4d78-8244-58268c312526",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_memory = model_size * param_size #TODO: Model Size Multiplied with Bytes per Parameter\n",
    "inference_outcome = memory_fit(inference_memory,CPU_RAM,GPU_RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89408375-8106-44e8-8214-e3c29c287129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of memory needed to load model for inference=\u001b[1m40.00 GB\u001b[0m\n",
      "\n",
      "Can this work on my setup?\n",
      "\u001b[1mYes, but fit needs both CPU and GPU\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f\"Amount of memory needed to load model for inference=\\033[1m{humanbytes(inference_memory)}\\033[0m\")\n",
    "print()\n",
    "print(f\"Can this work on my setup?\\n\\033[1m{inference_outcome}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad896f-ce57-463d-8dfc-6a9e551caab7",
   "metadata": {},
   "source": [
    "\n",
    "This is good for inference but we need to train/fine-tune this model.\n",
    "We need to accomodate for:\n",
    "- **Gradients/backpropagation** : Size same as model size\n",
    "- **Optimizer States** (ex: ADAM needs momentum and variance, can't be FP16): typically 12x of model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cea8e259-3f88-4304-8f04-889e24bc859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_params = model_size\n",
    "optimizer_memory = model_size*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7371150-4a51-4c94-a2da-54bdd382d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_memory = inference_memory + gradient_params + optimizer_memory\n",
    "finetune_outcome = memory_fit(finetune_memory,CPU_RAM,GPU_RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f9bc390-e1d5-4501-b733-5538b6d29ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of memory needed to load model for fintuning=\u001b[1m300.00 GB\u001b[0m\n",
      "\n",
      "Can this work on my setup?\n",
      "\u001b[1mNope, does not fit available memory\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f\"Amount of memory needed to load model for fintuning=\\033[1m{humanbytes(finetune_memory)}\\033[0m\")\n",
    "print()\n",
    "print(f\"Can this work on my setup?\\n\\033[1m{finetune_outcome}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07e0ca-4baa-423c-bc68-fe1726bb6de7",
   "metadata": {},
   "source": [
    "We need more memory (and faster GPUs). But just by usual scaling we would need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c60e7c9f-f5c7-4a64-a29f-e6c0fe6a63e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We Would need roughly need \u001b[1m8.0 more GPUs\u001b[0m to setup fine-tuning\n"
     ]
    }
   ],
   "source": [
    "additional_gpus = abs(-(finetune_memory - (CPU_RAM+GPU_RAM))//GPU_RAM) #TODO: HINT Required Memory / RAM per GPU\n",
    "print(f\"We Would need roughly need \\033[1m{additional_gpus} more GPUs\\033[0m to setup fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4aad26e-ac4c-4599-924a-c751dd08609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We Would spend roughly \u001b[1m$7.56/hr\u001b[0m to for fine-tuning with this setup\n"
     ]
    }
   ],
   "source": [
    "gpu = 'v100' # GPU RAM size is same for our example\n",
    "total_gpu_cost_per_hour = gpu_details.get(gpu).get('cost')*(additional_gpus+1)\n",
    "print(f\"We Would spend roughly \\033[1m${total_gpu_cost_per_hour}/hr\\033[0m to for fine-tuning with this setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf0559-95dc-47e7-95d9-d0c27b72daad",
   "metadata": {},
   "source": [
    "## Can We Bring Some Efficiencies? Please?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01698dd0-48b2-430b-a65f-272829a6cc93",
   "metadata": {},
   "source": [
    "Yes, luckily researchers have been exploring efficient ways of training and fine-tuning models to democratise availability of such models across a broader spectrum (and lower the environmental impact as well, yay!!!)\n",
    "\n",
    "We will focus on three key main efficiencies under the umbrella of **P**arameter **E**fficient **F**ine **T**uning paradigm:\n",
    "+ Quantization\n",
    "+ Additive PEFT\n",
    "    + Soft Prompting\n",
    "+ Reparameterization\n",
    "    + LoRA\n",
    " \n",
    "There are other techniques as well, such as, LoHa, LoKr, IA3, P-Tuning and more. While effective, the basic ideas are similar to what we will cover next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3557e-e975-465b-8b68-a3ff2181b719",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f9ce7-4a49-449a-a0d3-7c7cb33703a7",
   "metadata": {},
   "source": [
    "#### Basic Idea\n",
    "- The basic idea is to limit the number of bits/bytes needed to store our weights/parameters/matrix-cell values.\n",
    "- By default we store weights as high-precision floats in float32 or even float64 occupying enormous amounts of space (disk/memory).\n",
    "- Can we be smart and store those values without consuming all 32 or 64 bits?\n",
    "\n",
    "#### Let us understand this with a very simplified example:\n",
    "\n",
    "<img src=\"../assets/quantization.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00443452-6f5b-4d43-aceb-3ec2e48a9067",
   "metadata": {},
   "source": [
    "### Additive PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb24f59-ef79-49fb-bdd4-8535638b434e",
   "metadata": {},
   "source": [
    "Additive PEFT works by adding new tunable layers to the model and train only those during fine-tuning while keeping the base model weights frozen. \n",
    "\n",
    "But how do we do this?\n",
    "\n",
    "#### Soft Prompting\n",
    "- This technique involves introducing **task specific tokens** or **virtual tokens** to the model's input space\n",
    "- The virtual tokens are not part of the actual vocabulary of the model and only specify the task.\n",
    "- The dimensionality of virtual tokens is same as the input token embedding size\n",
    "- During finetuning the base model weights are frozen and only virtual token embedding layer is trained/updated.\n",
    "- Supports Mix-task Batch Fine-Tuning and hence no need for separate heads\n",
    "- Setup is similar to prefix tuning technique\n",
    "\n",
    "#### Overview of Soft Prompting\n",
    "\n",
    "<img src=\"../assets/soft_prompting_1.png\">\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../assets/soft_prompting_2.png\">\n",
    "\n",
    "---\n",
    "\n",
    "This technique is shown to be better/efficient and more stable than **manual prompting**.\n",
    "<img src=\"../assets/soft_prompting_perf.png\">\n",
    "\n",
    "> Source: https://arxiv.org/pdf/2104.08691\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc743b-0ebc-4de2-979d-786acddb6f40",
   "metadata": {},
   "source": [
    "### Reparametrization\n",
    "This technique smartly leverages matrix decomposition to bring efficiencies. Basically during fine-tuning:\n",
    "- we freeze the base model weights and\n",
    "- during the backward pass, we decompose the **weight update matrix** ($W_\\delta$) into two lower rank matrices $W_a$ and $W_b$ of rank $r$\n",
    "- This helps in achieving 100 to 1000x reduction in weights to be updated.\n",
    "\n",
    "The following illustration explains the same:\n",
    "<img src=\"../assets/lora_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c78ee-ee24-4517-9a09-649492edf75e",
   "metadata": {},
   "source": [
    "> qLoRA combines the improvements of quantization to LoRA thus leading to even further improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66f010-ff38-403e-8aef-2707ec3dc868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
